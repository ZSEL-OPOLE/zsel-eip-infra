# â˜¸ï¸ Kubernetes Cluster Architecture - ZSEL BCU

**JEDEN KLASTER K3S** - 9 Ã— Mac Pro M2 Ultra  
**Lokalizacja:** CPD Serwerownia BCU  
**Data:** 27 listopada 2025

---

## ğŸ“‹ Architektura Klastra

### Hardware

```text
9 Ã— Mac Pro M2 Ultra (2023)
â”œâ”€â”€ CPU:     24-core Apple Silicon M2 Ultra (ARM64)
â”œâ”€â”€ RAM:     192 GB unified memory
â”œâ”€â”€ Storage: 8 TB NVMe SSD (per node)
â””â”€â”€ Network: 2Ã— 10 Gbps Ethernet (OM4 fiber to Core Switch)

Total Resources:
- 216 CPU cores
- 1728 GB RAM
- 72 TB local storage (Longhorn distributed)
- 150 TB NAS storage (QNAP via NFS)
```

### Topologia

```text
Control Plane (HA):
â”œâ”€â”€ k3s-master-01  192.168.10.11  (etcd leader candidate)
â”œâ”€â”€ k3s-master-02  192.168.10.12  (etcd member)
â””â”€â”€ k3s-master-03  192.168.10.13  (etcd member)

Worker Nodes (Specialized):
â”œâ”€â”€ k3s-worker-01  192.168.10.14  [education]    â†’ Moodle, BBB, NextCloud
â”œâ”€â”€ k3s-worker-02  192.168.10.15  [education]    â†’ Mattermost, OnlyOffice
â”œâ”€â”€ k3s-worker-03  192.168.10.16  [devops]       â†’ GitLab, Harbor, Portainer
â”œâ”€â”€ k3s-worker-04  192.168.10.17  [ai-ml]        â†’ Ollama, JupyterHub, Qdrant
â”œâ”€â”€ k3s-worker-05  192.168.10.18  [analytics]    â†’ Zabbix, Prometheus, Grafana
â””â”€â”€ k3s-worker-06  192.168.10.19  [storage]      â†’ Longhorn, MinIO
```

---

## ğŸŒ Adresacja Sieciowa

### VLAN Structure (Kubernetes Dedicated)

**OBECNIE BRAK W vlans-master.yaml - DO DODANIA!**

| VLAN ID | Name              | Subnet           | Gateway       | Purpose           | Devices          |
|---------|-------------------|------------------|---------------|-------------------|------------------|
| **10** | K3s-Masters | 192.168.10.0/24 | 192.168.10.1 | Control Plane | 3 masters |
| **20** | K3s-Workers | 192.168.10.0/24 | 192.168.10.1 | Worker Nodes | 6 workers |
| **30** | K3s-MetalLB-PROD | 192.168.30.0/24 | - | LoadBalancer Pool | LoadBalancer IPs |
| **31** | K3s-MetalLB-DEV | 192.168.31.0/24 | - | DEV LoadBalancer | DEV environment |
| **32** | K3s-MetalLB-ADM | 192.168.32.0/24 | - | Admin LoadBalancer | Admin services |
| **40** | K3s-Storage | 192.168.40.0/24 | 192.168.40.1 | Longhorn iSCSI/NFS | Storage replication |
| **50** | K3s-VPN | 192.168.50.0/24 | 192.168.50.1 | WireGuard VPN | Remote access (100 users) |

**UWAGA:** W dokumentacji jest rÃ³Å¼na adresacja:
- `ARCHITEKTURA_CHMURY_AI.md`: 192.168.10.0/24 (jeden VLAN dla wszystkich)
- `03-VLAN-ADDRESSING.md`: VLAN 10=masters (10.10.10.0/24), VLAN 20=workers (10.10.20.0/24), etc.

**ZALECENIE:** Uproszczona struktura (jak w ARCHITEKTURA_CHMURY_AI.md):
```
VLAN 110: 192.168.10.0/24 - K3s Cluster (masters + workers)
  â”œâ”€â”€ .1        Gateway (CCR-BCU-01)
  â”œâ”€â”€ .11-.13   Masters (3)
  â”œâ”€â”€ .14-.19   Workers (6)
  â”œâ”€â”€ .20-.100  MetalLB pool PROD
  â”œâ”€â”€ .101-.150 MetalLB pool DEV
  â””â”€â”€ .200-.254 Reserved (WireGuard, management)
```

---

## ğŸ”— BGP Configuration (MetalLB)

### Router â†’ K3s Peering

**MikroTik CCR2216-BCU-01:**
```
AS: 65000
Router ID: 192.168.255.1 (management)

Peers:
â”œâ”€â”€ k3s-master-01  10.20.0.11  AS 65001  (TODO: Replace with actual IP)
â”œâ”€â”€ k3s-master-02  10.20.0.12  AS 65001
â””â”€â”€ k3s-master-03  10.20.0.13  AS 65001

Advertised Networks:
â”œâ”€â”€ 10.22.0.0/24   MetalLB PROD pool
â”œâ”€â”€ 10.12.0.0/24   MetalLB DEV pool
â”œâ”€â”€ 10.32.0.0/24   MetalLB ADM pool
â””â”€â”€ 10.20.0.0/22   K3s services network
```

**UWAGA:** BGP configuration w `vlans-master.yaml` uÅ¼ywa adresÃ³w 10.20.0.x, ale w VLAN structure nie ma takiej sieci!

**ZALECENIE:** Uproszczenie BGP do jednego pool:
```yaml
bgp:
  instance:
    as: 65000
    router_id: "192.168.255.1"
  
  peers:
    - name: "k3s-master-01"
      remote_address: "192.168.10.11"  # Zgodne z VLAN 110
      remote_as: 65001
    
    - name: "k3s-master-02"
      remote_address: "192.168.10.12"
      remote_as: 65001
    
    - name: "k3s-master-03"
      remote_address: "192.168.10.13"
      remote_as: 65001
  
  advertised_networks:
    - network: "192.168.10.20/27"  # .20-.51 = 32 IPs dla LoadBalancer PROD
      comment: "MetalLB LoadBalancer pool (Production)"
    
    - network: "192.168.10.101/26"  # .101-.150 = 50 IPs dla DEV
      comment: "MetalLB LoadBalancer pool (Development)"
```

---

## ğŸ’¾ Storage Architecture

### Longhorn Distributed Storage
```
Total: 40 TB usable (3Ã— replicas)
â”œâ”€â”€ Tier 1 (Critical): 10 TB  - PostgreSQL, FreeIPA, Keycloak
â”œâ”€â”€ Tier 2 (Standard): 20 TB  - Moodle, GitLab, Harbor, Mattermost
â””â”€â”€ Tier 3 (Bulk):     10 TB  - Backups, logs, media

Replication: 3Ã— (HA for critical services)
Snapshots: Daily incremental (S3 backup to MinIO)
Network: VLAN 40 (192.168.40.0/24) - iSCSI + NFS
```

### QNAP NAS (External)
```
Model: TS-h1277AXU-RP
Capacity: 150 TB usable (RAIDZ2)
IP: 192.168.20.10 (VLAN 20 - NAS)
Protocols: NFS, iSCSI
Purpose:
  â”œâ”€â”€ NextCloud storage (100 TB)
  â”œâ”€â”€ BigBlueButton recordings (30 TB)
  â”œâ”€â”€ Moodle course files (15 TB)
  â””â”€â”€ Backup destination (5 TB)
```

---

## ğŸ” Security & Access

### Network Policies (Zero Trust)
```
Default: DENY ALL

Allowed:
â”œâ”€â”€ Ingress: Only from Traefik (LoadBalancer)
â”œâ”€â”€ Egress: DNS (CoreDNS), external APIs (whitelist)
â”œâ”€â”€ Inter-namespace: Explicit allow only (280 policies total)
â””â”€â”€ Management: SSH/kubectl from VLAN 500 (admin) only

Blocked:
â”œâ”€â”€ Labs (VLAN 208-246) â†’ K3s cluster
â”œâ”€â”€ WiFi (VLAN 300-303) â†’ K3s cluster
â””â”€â”€ Students â†’ Control Plane (VLAN 110)
```

### WireGuard VPN (VLAN 50)
```
Subnet: 192.168.50.0/24
Server: 192.168.50.1 (runs on k3s-worker-03)
Clients: 100 concurrent
Purpose: Remote admin access (teachers, IT staff)
Routes: Full access to VLAN 500 (admin), read-only to K3s
```

### FreeIPA Integration
```
LDAP: ldap://freeipa.zsel.opole.pl
Base DN: dc=zsel,dc=opole,dc=pl
Users: 1030 (900 students + 100 teachers + 30 admin)
Groups:
  â”œâ”€â”€ cn=k8s-cluster-admins  â†’ ClusterRole: admin-full
  â”œâ”€â”€ cn=k8s-developers      â†’ ClusterRole: developer
  â””â”€â”€ cn=k8s-viewers         â†’ ClusterRole: viewer

SSO: Keycloak (25 apps integrated via OIDC/SAML)
```

---

## ğŸ“Š Applications (39 Total)

### Core Infrastructure (Wave 10)
| App | Namespace | Replicas | Resources | LoadBalancer IP |
|-----|-----------|----------|-----------|-----------------|
| MetalLB | core-network | - | 512Mi, 500m | - |
| Traefik | core-network | 2 | 2Gi, 1 CPU | 192.168.10.20 |
| FreeIPA | core-freeipa | 2 | 16Gi, 8 CPU | 192.168.10.21 |
| Keycloak | core-keycloak | 2 | 8Gi, 4 CPU | 192.168.10.22 |
| Longhorn | storage-system | DaemonSet | 36Gi, 18 CPU | - |
| CoreDNS | kube-system | 2 | 512Mi, 250m | - |

### Education (Wave 25)
| App | Namespace | Replicas | Resources | LoadBalancer IP |
|-----|-----------|----------|-----------|-----------------|
| Moodle | edu-moodle | 3 | 16Gi, 8 CPU | 192.168.10.30 |
| BigBlueButton | edu-bbb | 3 | 96Gi, 24 CPU | 192.168.10.31 |
| NextCloud | edu-nextcloud | 2 | 8Gi, 4 CPU | 192.168.10.32 |
| Mattermost | edu-mattermost | 2 | 16Gi, 4 CPU | 192.168.10.33 |
| OnlyOffice | edu-onlyoffice | 2 | 32Gi, 8 CPU | 192.168.10.34 |
| Etherpad | edu-etherpad | 2 | 8Gi, 2 CPU | 192.168.10.35 |
| Calibre-Web | edu-calibre | 1 | 4Gi, 1 CPU | 192.168.10.36 |

### DevOps (Wave 30)
| App | Namespace | Replicas | Resources | LoadBalancer IP |
|-----|-----------|----------|-----------|-----------------|
| GitLab | devops-gitlab | 1 | 32Gi, 8 CPU | 192.168.10.40 |
| Harbor | devops-harbor | 1 | 24Gi, 6 CPU | 192.168.10.41 |
| Portainer | admin-portainer | 1 | 4Gi, 1 CPU | 192.168.10.42 |

### AI/ML (Wave 40)
| App | Namespace | Replicas | Resources | LoadBalancer IP |
|-----|-----------|----------|-----------|-----------------|
| Ollama | ai-ml-ollama | 1 | 64Gi, 8 CPU | 192.168.10.50 |
| JupyterHub | ai-ml-jupyter | 1 | 16Gi, 2 CPU | 192.168.10.51 |
| Qdrant | ai-ml-qdrant | 2 | 64Gi, 8 CPU | 192.168.10.52 |

**TOTAL:** 39 apps, 47 namespaces, ~720 GB RAM, ~204 CPU cores

---

## ğŸš€ Deployment Process

### GitOps with ArgoCD
```
1. Git Push â†’ GitHub (zsel-eip-gitops)
2. ArgoCD detects change (2-minute poll)
3. Sync waves (0 â†’ 40):
   Wave 0:  ArgoCD Root (App-of-Apps)
   Wave 5:  Sealed Secrets Controller
   Wave 10: Core Infrastructure (6 apps)
   Wave 15: Security & Monitoring (10 apps)
   Wave 20: Databases (2 apps)
   Wave 25: Education (8 apps)
   Wave 30: DevOps + Communication (5 apps)
   Wave 40: AI/ML (3 apps)
4. Health checks & rollback if failed
5. Prometheus metrics + Grafana dashboards
```

### CI/CD Pipeline (GitHub Actions)
```
Stage 1: Pre-Validation (syntax, linting)
Stage 2: Security Scan (Trivy, kubesec, Gitleaks)
Stage 3: Quality Checks (kubeconform, OPA)
Stage 4: DEV Deployment + integration tests
Stage 5: Manual approval gate (PROD only, 2/3 approvers)
Stage 6: PROD Deployment (progressive sync)
Stage 7: Post-Validation (E2E, performance, security)
```

---

## ğŸ“ˆ Monitoring & Observability

### Metrics (Prometheus)
```
Targets: 300+
â”œâ”€â”€ 9 Mac Pro nodes (Node Exporter)
â”œâ”€â”€ 39 applications (ServiceMonitor)
â”œâ”€â”€ 57 MikroTik devices (SNMP Exporter)
â””â”€â”€ Kubernetes internals (kube-state-metrics)

Retention: 30 days (200 GB storage)
Scrape interval: 2 minutes
Alerting: Mattermost webhooks
```

### Logs (Loki)
```
Retention: 2 years (RODO compliance)
Storage: 500 GB
Collectors: Promtail (DaemonSet on 9 nodes)
Query: {namespace="edu-moodle"} |= "error"
```

### Infrastructure (Zabbix)
```
Hosts: 66 total
â”œâ”€â”€ 9 Mac Pro M2 Ultra nodes
â”œâ”€â”€ 57 MikroTik routers/switches
â””â”€â”€ 39 application health checks

Alerting: Email + Mattermost
Dashboard: 24/7 NOC display (VLAN 110)
```

---

## ğŸ”„ Backup & Disaster Recovery

### 4-Layer Strategy
```
1. Cluster State (Velero):
   - Daily full backup (etcd + manifests)
   - Hourly incremental
   - Retention: 90 days

2. Persistent Volumes (Longhorn):
   - Hourly snapshots
   - S3 backup to MinIO
   - Retention: 30 days

3. Databases (pg_dump/mysqldump):
   - Every 6 hours
   - Encrypted backups to QNAP NAS
   - Retention: 90 days

4. Offsite Replication:
   - Daily rsync to secondary location
   - 100 Mbps WireGuard tunnel
   - Retention: 1 year
```

### RTO/RPO
```
RTO (Recovery Time Objective): 4 hours
RPO (Recovery Point Objective): 6 hours

Recovery Procedure:
1. Restore etcd from Velero backup (30 min)
2. Restore PVCs from Longhorn snapshots (1 hour)
3. Restore databases from dumps (2 hours)
4. Verify health checks (30 min)
```

---

## ğŸ› Troubleshooting

### Common Issues

**1. Pod in CrashLoopBackOff**
```bash
kubectl describe pod <name> -n <namespace>
kubectl logs <name> -n <namespace> --previous
```

**2. PVC Pending**
```bash
kubectl get pvc -A | grep Pending
kubectl describe pvc <name> -n <namespace>
# Check: Longhorn operational, storage available
```

**3. LoadBalancer IP not assigned**
```bash
kubectl get svc -A | grep Pending
kubectl logs -n core-network -l app.kubernetes.io/name=metallb
# Check: BGP peering up (MikroTik â†” MetalLB)
```

**4. DNS resolution fails**
```bash
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup google.com
kubectl logs -n kube-system -l k8s-app=kube-dns
```

**5. Ingress not accessible**
```bash
kubectl get ingress -A
kubectl logs -n core-network -l app.kubernetes.io/name=traefik
# Check: Traefik pod running, LoadBalancer IP assigned
```

### Health Checks
```bash
# Cluster status
kubectl get nodes -o wide
kubectl cluster-info

# All applications
kubectl get applications -n argocd

# Pods not running
kubectl get pods -A | findstr -v "Running\|Completed"

# Storage
kubectl get pvc -A | findstr "Pending"
kubectl get sc

# Network
kubectl get svc -A | findstr "LoadBalancer"
kubectl get networkpolicies -A
```

---

## ğŸ“ Contact & Support

**Organization:** ZespÃ³Å‚ SzkÃ³Å‚ Elektronicznych i Logistycznych w Opolu  
**Team:** DevOps & Infrastructure  
**Email:** devops@zsel.opole.pl  
**GitHub:** https://github.com/zsel-opole/zsel-eip-gitops

**Emergency Contact:**
- On-call: +48 XXX XXX XXX
- Mattermost: @devops-team (24/7)

---

## ğŸ”— Related Documentation

| Document | Location | Description |
|----------|----------|-------------|
| GitOps Repository | `zsel-eip-gitops/` | ArgoCD manifests (39 apps) |
| Network Config | `zsel-eip-infra/` | VLAN, QoS, BGP (Terraform) |
| PFU Specification | `zsel-eip-dokumentacja/` | Program Funkcjonalno-UÅ¼ytkowy |
| Architecture Diagrams | `zsel-eip-dokumentacja/diagramy/` | Mermaid diagrams |

---

**Status:** âœ… Production Ready (1 klaster K3s)  
**Last updated:** 27 listopada 2025  
**Version:** 1.0.0
